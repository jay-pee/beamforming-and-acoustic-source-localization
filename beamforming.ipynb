{"cells":[{"cell_type":"markdown","source":[" # Basic of Beamforming and Source Localization with Steered response Power\n"," ## Motivation\n"," Beamforming is a technique to spatially filter out desired signal and surpress noise. This is applied in many different domains, like for example radar, mobile radio, hearing aids, speech enabled IoT devices.\n","\n"," ## Signal Model\n"," ![](2019-07-28-19-25-21.png)\n"," Model Description:\n"," $$\\underline{X}(\\Omega) = \\underline{A}^{\\text{ff}}(\\Omega) \\cdot S(\\Omega)$$\n"," ## Beamforming\n"," Beamforming or spatial filtering is an array processing technique used to improve the quality of the desired signal in the presence of noise. This filtering is accomplished by a linear combination of the recorded signals $X_m(\\Omega)$ and the beamformer weights $W_m(\\Omega)$. In other words, the filtered microphone signals are summed together (compare with figure below). When the filter weights are configured correctly, the desired signal is superimposed constructively.\n"," ![](2019-07-28-14-30-39.png)\n"," Image shows a filter and sum beamformer.\tMicrophone signals $\\underline{X}(\\Omega)$ are multiplied with the beamformer weights $\\underline{W}(\\Omega)$ and then accumulated to the beamformer output signal $Y(\\Omega)$.\n"," $$Y(\\Omega) = \\underline{W}^\\text{H}(\\Omega) \\cdot \\underline{X}(\\Omega)$$"],"metadata":{}},{"source":["# Imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","np.set_printoptions(precision=3)\n","def H(A, **kwargs):\n","    return np.transpose(A,**kwargs).conj()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# ##Parameter\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["varphi = 45 / 180 * np.pi # Angle of attack of the Source S(\\Omega) in relation to the mic array \n","c = 343000 # Velocity of sound in mm/s\n","mic = 6 # count of mics\n","d = 20 # distance in mm\n","fs = 16000 # Sample rate\n","\n","n_fft = 512 # Fourier Transform length\n","n_spec = 257 # Number of frequency bins \n","n_dir = 72 # Number of directions which the steering vector is steered to\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ##Microphone Positions\n"," `pos_y` and `pos_x` are the microphone positions. It is a Uniform Linear Array (ULA) type (like seen in the Figure below)"],"metadata":{}},{"source":["pos_y = np.zeros((1,mic))\n","pos_x = np.r_[0.:mic]*d\n","\n","fig, ax = plt.subplots()\n","ax.scatter(pos_x, pos_y, c='tab:red', alpha=1, edgecolors='white')\n","plt.ylabel('Y Position [mm]')\n","plt.xlabel('X Position [mm]')\n","plt.ylim((-50, 50))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Free Field model and delay vectors\n"," ...\n","$$\\underline A_q^{\\text{ff}}(\\Omega) = \\exp\\big(-j\\Omega f_s \\Delta\\underline \\tau(\\varphi_q)\\big),$$\n"," Calculate the delay vectors to each microphone to the source $q$ in the frequency domain:"],"metadata":{}},{"source":["tau = (pos_x*np.cos(varphi)+pos_y*np.sin(varphi))/c #calculating delay vector tau (in the time domain) depending on the array geometry.\n","tau = tau.reshape([mic,1,1])\n","Omega_array = np.r_[0.:n_spec].T*np.pi/n_fft*2\n","Omega_array = Omega_array.reshape([1,1,n_spec])\n","A_ff = np.exp(-1j*Omega_array*fs*tau)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["tmp = np.squeeze(np.round(np.angle(A_ff[:,:,:])/np.pi*180))\n","plt.plot(tmp.T)\n","plt.ylabel(\"Angle [Deg]\")\n","plt.xlabel(\"Frequency [Bin]\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" The plot shows the angle of the complex spectral time delays from the desired signal between reference microphone 1 and the others. for higher frequencys you see that the angle is growing due to the faster swinging of the signal. This means for the same time delay different frequencys have different phase differences between two microphones.\n"," ## Acoustic Sound Localization\n"," Acoustic sound localization is the task of locating a sound source given measurements of the sound field. ...\n"," ## Calculate the steering vectors W_H for the localization:"],"metadata":{}},{"cell_type":"markdown","source":[" ## Spatial Convariance\n"," Another important signal property is the covariance that describes the interdependencies between the microphone signals $\\underline X(\\Omega)$. To obtain this covariance, it is presumed that the signals are stochastic. When only considering one source ($Q=1$),\n"," the spatial covariance matrix can be denoted as\n"," $$\\mathbf \\Phi_{xx}(\\Omega) = \\text{E}\\{\\underline X(\\Omega)\\underline X^H(\\Omega)\\}$$\n"," $$ = \\underline A(\\Omega) \\text{E} \\{ S'(\\Omega)  S'^*(\\Omega)\\}\\underline A^H(\\Omega) + \\text{E}\\{\\underline V(\\Omega)\\underline V^H(\\Omega)\\}$$\n"," $$ = \\mathbf \\Phi_{ss}(\\Omega) + \\mathbf \\Phi_{vv}(\\Omega),$$\n"," where $E\\{\\cdot\\}$ represents the expectation value operator, $^*$ denotes the complex conjugate operator, $\\mathbf \\Phi_{ss}(\\Omega)$ represents the source correlation matrix, $\\mathbf \\Phi_{vv}(\\Omega)$ the noise correlation matrix and $(\\cdot)^H$ the Hermitean operator.\n"," If we consider noise not present $V=0$ and the expectation value of the signal $\\text{E}{S(\\Omega)}=1$ then the formular for the spatial covariance matrix $\\mathbf \\Phi_{xx}(\\Omega)$ reduces to\n"," $$\\mathbf \\Phi_{xx}(\\Omega) =  \\underline A(\\Omega) \\underline A^H(\\Omega) $$"],"metadata":{}},{"source":["A_ff_H = A_ff.reshape([1,mic,n_spec]).copy()\n","A_ff_H = A_ff_H.conj()\n","phi_xx = A_ff_H * A_ff\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df = pd.DataFrame(phi_xx[:,:,50])\n","df.style.format('{:,.2f}'.format)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["power_steered = np.zeros((n_spec,n_dir))\n","for iDir in range(n_dir):\n","    for iF in range(n_spec):\n","        tmp = np.dot(W_H[:,:,iF,iDir], phi_xx[:,:,iF])\n","        power_steered[iF,iDir] = np.abs(np.dot(tmp, W[:,:,iF,iDir]))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plt.figure(1)\n","plt.imshow(power_steered, aspect='auto', origin='lower')\n","plt.show()\n","\n","# with pd.option_context('display.precision', 3):\n","# pd.set_option('precision', 0)\n","# pd.set_option('display.float_format', lambda x: '%.0f' % x)\n","# df = pd.DataFrame(power_steered)\n","# df.style.format('{:,.2f}'.format)\n","# print(df)\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}